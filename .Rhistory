knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(classe ~  ., data = bs_training, method = "class")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(classe ~  ., data = bs_training, method = "class")
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(classe ~  ., data = bs_training, method = "class")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(classe ~  ., data = bs_training, method = "class")
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(classe ~  ., data = bs_training, method = "class")
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(as.factor(classe) ~  ., data = bs_training, method = "class")
rando_forest_predict <- predict( rando_forest, bs_testing, type = "class")
# Regression Tree Matrix
reg_tree_cm <- confusionMatrix(reg_tree_predict, bs_testing$classe)
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(as.factor(classe) ~  ., data = bs_training, method = "class")
rando_forest_predict <- predict( rando_forest, bs_testing, type = "class")
# Regression Tree Matrix
reg_tree_cm <- confusionMatrix(factor(reg_tree_predict), bs_testing$classe)
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(randomForest); library(rpart); library(rpart.plot)
set.seed(2018)
# Read CSVs (after download, of course) while dealing with NAs
training_data <- read.csv("./pml-training.csv", na.strings = c("NA","","#DIV/0!"))
testing_data <- read.csv("./pml-testing.csv", na.strings = c("NA","","#DIV/0!"))
str(training_data)
training_data <- training_data[, colSums(is.na(training_data)) == 0]
testing_data <- testing_data[, colSums(is.na(testing_data)) == 0]
names(training_data) # the first 7 columns won' be needed
training_data <- training_data[ , -c(1:7)]
testing_data  <- testing_data[ , -c(1:7)]
summary(training_data) # looks like all variables have useful summaries
table(is.na(training_data)) # We don't want to see any TRUE's here
bs_sample <- createDataPartition( y = training_data$classe, p = 0.7, list = FALSE)
bs_training <- training_data[  bs_sample, ] # include bootstrap sample
bs_testing  <- training_data[ -bs_sample, ] # exclude bootstrap sample
# Spot-check
head(bs_training)
barplot(table(training_data$classe), main = "Distribution of Training Boostrap Classes (5 Levels)")
# Recursive Partitioing and Regression Tree
reg_tree <- rpart( classe ~ ., data = bs_training, method = "class" )
# Model Predictions
reg_tree_predict <- predict( reg_tree, bs_testing, type = "class")
# Visualize
rpart.plot(reg_tree, main = "Plot of the rpart model (tree)")
rando_forest <- randomForest(as.factor(classe) ~  ., data = bs_training, method = "class")
rando_forest_predict <- predict( rando_forest, bs_testing, type = "class")
# Regression Tree Matrix
reg_tree_cm <- confusionMatrix(factor(reg_tree_predict), factor(bs_testing$classe))
reg_tree_accuracy <- reg_tree_cm$overall['Accuracy']
# Random Forest Matrix
rando_forest_cm <- confusionMatrix(factor(rando_forest_predict), factor(bs_testing$classe))
rando_forest_accuracy <- rando_forest_cm$overall['Accuracy']
print(paste0("Regression Tree Accuracy: ", reg_tree_accuracy))
print(paste0("Random Forest Accuracy: ", rando_forest_accuracy))
